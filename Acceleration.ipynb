{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Num GPUs Available: ', len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df  = pd.read_csv(\"Z:/Divya/TEMP_transfers/toAni/BPN_P9LT_P9RT_flyCoords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stepcycle_predictions(df, joint_angles, velocity):\n",
    "\n",
    "    if joint_angles == True:\n",
    "        terms = ['D_flex', 'ball', '_r', 'cycle','tnum','fnum','flynum', 'SF', 'pos', '_x', '_y', '_z', 'rot', 'abduct']\n",
    "    if velocity == True:\n",
    "        terms = ['D_flex', 'ball', '_r', 'cycle', 'genotype', 'SF', 'pos', '_x', '_y', '_z']\n",
    "    \n",
    "    for i in terms:\n",
    "        cols = [c for c in df.columns if c.endswith(i)]\n",
    "\n",
    "        df = df.drop(columns=cols)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_frames(df, f_0=400, f_f=1000, f_trl=1400):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df_filt = df.loc[\n",
    "        ((df.loc[:, \"fnum\"] % f_trl) >= f_0) & ((df.loc[:, \"fnum\"] % f_trl) < f_f)\n",
    "    ]\n",
    "\n",
    "    return df_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_scoring(df):\n",
    "    from scipy import stats\n",
    "\n",
    "    for column in df.columns.to_list():\n",
    "        if column not in ['x_vel', 'y_vel', 'z_vel']:\n",
    "            df[column] = stats.zscore(df[column])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L1A_flex</th>\n",
       "      <th>L1B_flex</th>\n",
       "      <th>L1C_flex</th>\n",
       "      <th>L2A_flex</th>\n",
       "      <th>L2B_flex</th>\n",
       "      <th>L2C_flex</th>\n",
       "      <th>L3A_flex</th>\n",
       "      <th>L3B_flex</th>\n",
       "      <th>L3C_flex</th>\n",
       "      <th>R1A_flex</th>\n",
       "      <th>...</th>\n",
       "      <th>R1C_flex</th>\n",
       "      <th>R2A_flex</th>\n",
       "      <th>R2B_flex</th>\n",
       "      <th>R2C_flex</th>\n",
       "      <th>R3A_flex</th>\n",
       "      <th>R3B_flex</th>\n",
       "      <th>R3C_flex</th>\n",
       "      <th>x_vel</th>\n",
       "      <th>y_vel</th>\n",
       "      <th>z_vel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305200</th>\n",
       "      <td>0.758239</td>\n",
       "      <td>0.269748</td>\n",
       "      <td>0.714661</td>\n",
       "      <td>-0.539818</td>\n",
       "      <td>-0.733766</td>\n",
       "      <td>-1.731932</td>\n",
       "      <td>0.056881</td>\n",
       "      <td>0.282390</td>\n",
       "      <td>-0.310062</td>\n",
       "      <td>-0.264968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693084</td>\n",
       "      <td>-0.585737</td>\n",
       "      <td>-1.094131</td>\n",
       "      <td>-1.513160</td>\n",
       "      <td>1.320046</td>\n",
       "      <td>-0.004703</td>\n",
       "      <td>0.497005</td>\n",
       "      <td>1.096183</td>\n",
       "      <td>2.616862</td>\n",
       "      <td>5.257112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305201</th>\n",
       "      <td>0.831051</td>\n",
       "      <td>0.261139</td>\n",
       "      <td>0.636890</td>\n",
       "      <td>-0.576519</td>\n",
       "      <td>-0.610017</td>\n",
       "      <td>-1.720330</td>\n",
       "      <td>0.174117</td>\n",
       "      <td>0.172505</td>\n",
       "      <td>-0.249874</td>\n",
       "      <td>-0.271291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694916</td>\n",
       "      <td>-0.579001</td>\n",
       "      <td>-1.113678</td>\n",
       "      <td>-1.506804</td>\n",
       "      <td>0.212285</td>\n",
       "      <td>-0.272115</td>\n",
       "      <td>0.589078</td>\n",
       "      <td>1.186000</td>\n",
       "      <td>2.676443</td>\n",
       "      <td>5.157295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305202</th>\n",
       "      <td>0.824661</td>\n",
       "      <td>0.225952</td>\n",
       "      <td>0.555822</td>\n",
       "      <td>-0.619366</td>\n",
       "      <td>-0.503481</td>\n",
       "      <td>-1.727147</td>\n",
       "      <td>0.177643</td>\n",
       "      <td>0.132534</td>\n",
       "      <td>-0.203036</td>\n",
       "      <td>-0.267053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664057</td>\n",
       "      <td>-0.530863</td>\n",
       "      <td>-1.159401</td>\n",
       "      <td>-1.499572</td>\n",
       "      <td>-0.320635</td>\n",
       "      <td>-0.432969</td>\n",
       "      <td>0.692877</td>\n",
       "      <td>1.273659</td>\n",
       "      <td>2.689129</td>\n",
       "      <td>5.039348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305203</th>\n",
       "      <td>0.768999</td>\n",
       "      <td>0.160785</td>\n",
       "      <td>0.460894</td>\n",
       "      <td>-0.693495</td>\n",
       "      <td>-0.411975</td>\n",
       "      <td>-1.742215</td>\n",
       "      <td>0.167572</td>\n",
       "      <td>0.133926</td>\n",
       "      <td>-0.180630</td>\n",
       "      <td>-0.235028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604496</td>\n",
       "      <td>-0.430864</td>\n",
       "      <td>-1.228570</td>\n",
       "      <td>-1.493661</td>\n",
       "      <td>-0.468435</td>\n",
       "      <td>-0.493286</td>\n",
       "      <td>0.799473</td>\n",
       "      <td>1.358636</td>\n",
       "      <td>2.643880</td>\n",
       "      <td>4.900506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305204</th>\n",
       "      <td>0.701472</td>\n",
       "      <td>0.088044</td>\n",
       "      <td>0.365363</td>\n",
       "      <td>-0.797573</td>\n",
       "      <td>-0.335046</td>\n",
       "      <td>-1.749145</td>\n",
       "      <td>0.162281</td>\n",
       "      <td>0.143477</td>\n",
       "      <td>-0.183611</td>\n",
       "      <td>-0.192129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541270</td>\n",
       "      <td>-0.281702</td>\n",
       "      <td>-1.303932</td>\n",
       "      <td>-1.493818</td>\n",
       "      <td>-0.461853</td>\n",
       "      <td>-0.511329</td>\n",
       "      <td>0.897564</td>\n",
       "      <td>1.440223</td>\n",
       "      <td>2.531357</td>\n",
       "      <td>4.738617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487195</th>\n",
       "      <td>-0.308088</td>\n",
       "      <td>-0.902133</td>\n",
       "      <td>-0.788416</td>\n",
       "      <td>-0.455089</td>\n",
       "      <td>-0.965098</td>\n",
       "      <td>-0.731755</td>\n",
       "      <td>1.814973</td>\n",
       "      <td>-0.622985</td>\n",
       "      <td>-0.564088</td>\n",
       "      <td>1.669204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589357</td>\n",
       "      <td>-1.684049</td>\n",
       "      <td>0.313072</td>\n",
       "      <td>0.856602</td>\n",
       "      <td>-0.335242</td>\n",
       "      <td>2.837208</td>\n",
       "      <td>2.588538</td>\n",
       "      <td>-0.428126</td>\n",
       "      <td>0.975551</td>\n",
       "      <td>0.811247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487196</th>\n",
       "      <td>-0.303594</td>\n",
       "      <td>-0.903629</td>\n",
       "      <td>-0.788044</td>\n",
       "      <td>-0.471002</td>\n",
       "      <td>-0.972361</td>\n",
       "      <td>-0.740052</td>\n",
       "      <td>1.767146</td>\n",
       "      <td>-0.655100</td>\n",
       "      <td>-0.613778</td>\n",
       "      <td>1.632109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533910</td>\n",
       "      <td>-1.358959</td>\n",
       "      <td>0.320817</td>\n",
       "      <td>0.854324</td>\n",
       "      <td>-0.325824</td>\n",
       "      <td>2.827270</td>\n",
       "      <td>2.594308</td>\n",
       "      <td>-0.656853</td>\n",
       "      <td>1.225061</td>\n",
       "      <td>0.704523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487197</th>\n",
       "      <td>-0.297195</td>\n",
       "      <td>-0.909914</td>\n",
       "      <td>-0.786307</td>\n",
       "      <td>-0.484294</td>\n",
       "      <td>-0.982479</td>\n",
       "      <td>-0.748602</td>\n",
       "      <td>1.729157</td>\n",
       "      <td>-0.692736</td>\n",
       "      <td>-0.674416</td>\n",
       "      <td>1.598554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495282</td>\n",
       "      <td>-0.973195</td>\n",
       "      <td>0.329923</td>\n",
       "      <td>0.847678</td>\n",
       "      <td>-0.320627</td>\n",
       "      <td>2.818182</td>\n",
       "      <td>2.595073</td>\n",
       "      <td>-0.860701</td>\n",
       "      <td>1.480759</td>\n",
       "      <td>0.588388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487198</th>\n",
       "      <td>-0.290678</td>\n",
       "      <td>-0.922501</td>\n",
       "      <td>-0.782228</td>\n",
       "      <td>-0.496396</td>\n",
       "      <td>-0.994606</td>\n",
       "      <td>-0.753511</td>\n",
       "      <td>1.710593</td>\n",
       "      <td>-0.723309</td>\n",
       "      <td>-0.729794</td>\n",
       "      <td>1.569301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466783</td>\n",
       "      <td>-0.970619</td>\n",
       "      <td>0.335979</td>\n",
       "      <td>0.857931</td>\n",
       "      <td>-0.321187</td>\n",
       "      <td>2.814204</td>\n",
       "      <td>2.593385</td>\n",
       "      <td>-1.038202</td>\n",
       "      <td>1.732315</td>\n",
       "      <td>0.469502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487199</th>\n",
       "      <td>-0.286750</td>\n",
       "      <td>-0.940638</td>\n",
       "      <td>-0.773509</td>\n",
       "      <td>-0.511405</td>\n",
       "      <td>-1.008118</td>\n",
       "      <td>-0.752298</td>\n",
       "      <td>1.712542</td>\n",
       "      <td>-0.740850</td>\n",
       "      <td>-0.771067</td>\n",
       "      <td>1.545161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444080</td>\n",
       "      <td>-1.614606</td>\n",
       "      <td>0.335653</td>\n",
       "      <td>0.899181</td>\n",
       "      <td>-0.328250</td>\n",
       "      <td>2.816906</td>\n",
       "      <td>2.590686</td>\n",
       "      <td>-1.190549</td>\n",
       "      <td>1.972467</td>\n",
       "      <td>0.352829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        L1A_flex  L1B_flex  L1C_flex  L2A_flex  L2B_flex  L2C_flex  L3A_flex  \\\n",
       "305200  0.758239  0.269748  0.714661 -0.539818 -0.733766 -1.731932  0.056881   \n",
       "305201  0.831051  0.261139  0.636890 -0.576519 -0.610017 -1.720330  0.174117   \n",
       "305202  0.824661  0.225952  0.555822 -0.619366 -0.503481 -1.727147  0.177643   \n",
       "305203  0.768999  0.160785  0.460894 -0.693495 -0.411975 -1.742215  0.167572   \n",
       "305204  0.701472  0.088044  0.365363 -0.797573 -0.335046 -1.749145  0.162281   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "487195 -0.308088 -0.902133 -0.788416 -0.455089 -0.965098 -0.731755  1.814973   \n",
       "487196 -0.303594 -0.903629 -0.788044 -0.471002 -0.972361 -0.740052  1.767146   \n",
       "487197 -0.297195 -0.909914 -0.786307 -0.484294 -0.982479 -0.748602  1.729157   \n",
       "487198 -0.290678 -0.922501 -0.782228 -0.496396 -0.994606 -0.753511  1.710593   \n",
       "487199 -0.286750 -0.940638 -0.773509 -0.511405 -1.008118 -0.752298  1.712542   \n",
       "\n",
       "        L3B_flex  L3C_flex  R1A_flex  ...  R1C_flex  R2A_flex  R2B_flex  \\\n",
       "305200  0.282390 -0.310062 -0.264968  ...  0.693084 -0.585737 -1.094131   \n",
       "305201  0.172505 -0.249874 -0.271291  ...  0.694916 -0.579001 -1.113678   \n",
       "305202  0.132534 -0.203036 -0.267053  ...  0.664057 -0.530863 -1.159401   \n",
       "305203  0.133926 -0.180630 -0.235028  ...  0.604496 -0.430864 -1.228570   \n",
       "305204  0.143477 -0.183611 -0.192129  ...  0.541270 -0.281702 -1.303932   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "487195 -0.622985 -0.564088  1.669204  ...  0.589357 -1.684049  0.313072   \n",
       "487196 -0.655100 -0.613778  1.632109  ...  0.533910 -1.358959  0.320817   \n",
       "487197 -0.692736 -0.674416  1.598554  ...  0.495282 -0.973195  0.329923   \n",
       "487198 -0.723309 -0.729794  1.569301  ...  0.466783 -0.970619  0.335979   \n",
       "487199 -0.740850 -0.771067  1.545161  ...  0.444080 -1.614606  0.335653   \n",
       "\n",
       "        R2C_flex  R3A_flex  R3B_flex  R3C_flex     x_vel     y_vel     z_vel  \n",
       "305200 -1.513160  1.320046 -0.004703  0.497005  1.096183  2.616862  5.257112  \n",
       "305201 -1.506804  0.212285 -0.272115  0.589078  1.186000  2.676443  5.157295  \n",
       "305202 -1.499572 -0.320635 -0.432969  0.692877  1.273659  2.689129  5.039348  \n",
       "305203 -1.493661 -0.468435 -0.493286  0.799473  1.358636  2.643880  4.900506  \n",
       "305204 -1.493818 -0.461853 -0.511329  0.897564  1.440223  2.531357  4.738617  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "487195  0.856602 -0.335242  2.837208  2.588538 -0.428126  0.975551  0.811247  \n",
       "487196  0.854324 -0.325824  2.827270  2.594308 -0.656853  1.225061  0.704523  \n",
       "487197  0.847678 -0.320627  2.818182  2.595073 -0.860701  1.480759  0.588388  \n",
       "487198  0.857931 -0.321187  2.814204  2.593385 -1.038202  1.732315  0.469502  \n",
       "487199  0.899181 -0.328250  2.816906  2.590686 -1.190549  1.972467  0.352829  \n",
       "\n",
       "[182000 rows x 21 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = filter_frames(df)\n",
    "df = df.reset_index()\n",
    "df  = remove_stepcycle_predictions(df, joint_angles=True, velocity=False)\n",
    "df = df[df['genotype']=='P9RT']\n",
    "df  = df.drop(columns=['index','genotype'])\n",
    "df = z_scoring(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_lagged_features(data, features, lag):\n",
    "    df = data.copy()\n",
    "    for feature in features:\n",
    "        df[f'{feature}_lag_{lag}'] = df[feature].shift(lag)\n",
    "        df = df.drop(columns=[f'{feature}'])\n",
    "    df.dropna(inplace=True)  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>L1A_flex</th>\n",
       "      <th>L1B_flex</th>\n",
       "      <th>L1C_flex</th>\n",
       "      <th>L2A_flex</th>\n",
       "      <th>L2B_flex</th>\n",
       "      <th>L2C_flex</th>\n",
       "      <th>L3A_flex</th>\n",
       "      <th>L3B_flex</th>\n",
       "      <th>L3C_flex</th>\n",
       "      <th>R1A_flex</th>\n",
       "      <th>...</th>\n",
       "      <th>R1C_flex</th>\n",
       "      <th>R2A_flex</th>\n",
       "      <th>R2B_flex</th>\n",
       "      <th>R2C_flex</th>\n",
       "      <th>R3A_flex</th>\n",
       "      <th>R3B_flex</th>\n",
       "      <th>R3C_flex</th>\n",
       "      <th>x_vel</th>\n",
       "      <th>y_vel</th>\n",
       "      <th>z_vel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305200</th>\n",
       "      <td>0.758239</td>\n",
       "      <td>0.269748</td>\n",
       "      <td>0.714661</td>\n",
       "      <td>-0.539818</td>\n",
       "      <td>-0.733766</td>\n",
       "      <td>-1.731932</td>\n",
       "      <td>0.056881</td>\n",
       "      <td>0.282390</td>\n",
       "      <td>-0.310062</td>\n",
       "      <td>-0.264968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693084</td>\n",
       "      <td>-0.585737</td>\n",
       "      <td>-1.094131</td>\n",
       "      <td>-1.513160</td>\n",
       "      <td>1.320046</td>\n",
       "      <td>-0.004703</td>\n",
       "      <td>0.497005</td>\n",
       "      <td>1.096183</td>\n",
       "      <td>2.616862</td>\n",
       "      <td>5.257112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305201</th>\n",
       "      <td>0.831051</td>\n",
       "      <td>0.261139</td>\n",
       "      <td>0.636890</td>\n",
       "      <td>-0.576519</td>\n",
       "      <td>-0.610017</td>\n",
       "      <td>-1.720330</td>\n",
       "      <td>0.174117</td>\n",
       "      <td>0.172505</td>\n",
       "      <td>-0.249874</td>\n",
       "      <td>-0.271291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694916</td>\n",
       "      <td>-0.579001</td>\n",
       "      <td>-1.113678</td>\n",
       "      <td>-1.506804</td>\n",
       "      <td>0.212285</td>\n",
       "      <td>-0.272115</td>\n",
       "      <td>0.589078</td>\n",
       "      <td>1.186000</td>\n",
       "      <td>2.676443</td>\n",
       "      <td>5.157295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305202</th>\n",
       "      <td>0.824661</td>\n",
       "      <td>0.225952</td>\n",
       "      <td>0.555822</td>\n",
       "      <td>-0.619366</td>\n",
       "      <td>-0.503481</td>\n",
       "      <td>-1.727147</td>\n",
       "      <td>0.177643</td>\n",
       "      <td>0.132534</td>\n",
       "      <td>-0.203036</td>\n",
       "      <td>-0.267053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664057</td>\n",
       "      <td>-0.530863</td>\n",
       "      <td>-1.159401</td>\n",
       "      <td>-1.499572</td>\n",
       "      <td>-0.320635</td>\n",
       "      <td>-0.432969</td>\n",
       "      <td>0.692877</td>\n",
       "      <td>1.273659</td>\n",
       "      <td>2.689129</td>\n",
       "      <td>5.039348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305203</th>\n",
       "      <td>0.768999</td>\n",
       "      <td>0.160785</td>\n",
       "      <td>0.460894</td>\n",
       "      <td>-0.693495</td>\n",
       "      <td>-0.411975</td>\n",
       "      <td>-1.742215</td>\n",
       "      <td>0.167572</td>\n",
       "      <td>0.133926</td>\n",
       "      <td>-0.180630</td>\n",
       "      <td>-0.235028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604496</td>\n",
       "      <td>-0.430864</td>\n",
       "      <td>-1.228570</td>\n",
       "      <td>-1.493661</td>\n",
       "      <td>-0.468435</td>\n",
       "      <td>-0.493286</td>\n",
       "      <td>0.799473</td>\n",
       "      <td>1.358636</td>\n",
       "      <td>2.643880</td>\n",
       "      <td>4.900506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305204</th>\n",
       "      <td>0.701472</td>\n",
       "      <td>0.088044</td>\n",
       "      <td>0.365363</td>\n",
       "      <td>-0.797573</td>\n",
       "      <td>-0.335046</td>\n",
       "      <td>-1.749145</td>\n",
       "      <td>0.162281</td>\n",
       "      <td>0.143477</td>\n",
       "      <td>-0.183611</td>\n",
       "      <td>-0.192129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541270</td>\n",
       "      <td>-0.281702</td>\n",
       "      <td>-1.303932</td>\n",
       "      <td>-1.493818</td>\n",
       "      <td>-0.461853</td>\n",
       "      <td>-0.511329</td>\n",
       "      <td>0.897564</td>\n",
       "      <td>1.440223</td>\n",
       "      <td>2.531357</td>\n",
       "      <td>4.738617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487195</th>\n",
       "      <td>-0.308088</td>\n",
       "      <td>-0.902133</td>\n",
       "      <td>-0.788416</td>\n",
       "      <td>-0.455089</td>\n",
       "      <td>-0.965098</td>\n",
       "      <td>-0.731755</td>\n",
       "      <td>1.814973</td>\n",
       "      <td>-0.622985</td>\n",
       "      <td>-0.564088</td>\n",
       "      <td>1.669204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589357</td>\n",
       "      <td>-1.684049</td>\n",
       "      <td>0.313072</td>\n",
       "      <td>0.856602</td>\n",
       "      <td>-0.335242</td>\n",
       "      <td>2.837208</td>\n",
       "      <td>2.588538</td>\n",
       "      <td>-0.428126</td>\n",
       "      <td>0.975551</td>\n",
       "      <td>0.811247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487196</th>\n",
       "      <td>-0.303594</td>\n",
       "      <td>-0.903629</td>\n",
       "      <td>-0.788044</td>\n",
       "      <td>-0.471002</td>\n",
       "      <td>-0.972361</td>\n",
       "      <td>-0.740052</td>\n",
       "      <td>1.767146</td>\n",
       "      <td>-0.655100</td>\n",
       "      <td>-0.613778</td>\n",
       "      <td>1.632109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533910</td>\n",
       "      <td>-1.358959</td>\n",
       "      <td>0.320817</td>\n",
       "      <td>0.854324</td>\n",
       "      <td>-0.325824</td>\n",
       "      <td>2.827270</td>\n",
       "      <td>2.594308</td>\n",
       "      <td>-0.656853</td>\n",
       "      <td>1.225061</td>\n",
       "      <td>0.704523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487197</th>\n",
       "      <td>-0.297195</td>\n",
       "      <td>-0.909914</td>\n",
       "      <td>-0.786307</td>\n",
       "      <td>-0.484294</td>\n",
       "      <td>-0.982479</td>\n",
       "      <td>-0.748602</td>\n",
       "      <td>1.729157</td>\n",
       "      <td>-0.692736</td>\n",
       "      <td>-0.674416</td>\n",
       "      <td>1.598554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495282</td>\n",
       "      <td>-0.973195</td>\n",
       "      <td>0.329923</td>\n",
       "      <td>0.847678</td>\n",
       "      <td>-0.320627</td>\n",
       "      <td>2.818182</td>\n",
       "      <td>2.595073</td>\n",
       "      <td>-0.860701</td>\n",
       "      <td>1.480759</td>\n",
       "      <td>0.588388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487198</th>\n",
       "      <td>-0.290678</td>\n",
       "      <td>-0.922501</td>\n",
       "      <td>-0.782228</td>\n",
       "      <td>-0.496396</td>\n",
       "      <td>-0.994606</td>\n",
       "      <td>-0.753511</td>\n",
       "      <td>1.710593</td>\n",
       "      <td>-0.723309</td>\n",
       "      <td>-0.729794</td>\n",
       "      <td>1.569301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466783</td>\n",
       "      <td>-0.970619</td>\n",
       "      <td>0.335979</td>\n",
       "      <td>0.857931</td>\n",
       "      <td>-0.321187</td>\n",
       "      <td>2.814204</td>\n",
       "      <td>2.593385</td>\n",
       "      <td>-1.038202</td>\n",
       "      <td>1.732315</td>\n",
       "      <td>0.469502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487199</th>\n",
       "      <td>-0.286750</td>\n",
       "      <td>-0.940638</td>\n",
       "      <td>-0.773509</td>\n",
       "      <td>-0.511405</td>\n",
       "      <td>-1.008118</td>\n",
       "      <td>-0.752298</td>\n",
       "      <td>1.712542</td>\n",
       "      <td>-0.740850</td>\n",
       "      <td>-0.771067</td>\n",
       "      <td>1.545161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444080</td>\n",
       "      <td>-1.614606</td>\n",
       "      <td>0.335653</td>\n",
       "      <td>0.899181</td>\n",
       "      <td>-0.328250</td>\n",
       "      <td>2.816906</td>\n",
       "      <td>2.590686</td>\n",
       "      <td>-1.190549</td>\n",
       "      <td>1.972467</td>\n",
       "      <td>0.352829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        L1A_flex  L1B_flex  L1C_flex  L2A_flex  L2B_flex  L2C_flex  L3A_flex  \\\n",
       "305200  0.758239  0.269748  0.714661 -0.539818 -0.733766 -1.731932  0.056881   \n",
       "305201  0.831051  0.261139  0.636890 -0.576519 -0.610017 -1.720330  0.174117   \n",
       "305202  0.824661  0.225952  0.555822 -0.619366 -0.503481 -1.727147  0.177643   \n",
       "305203  0.768999  0.160785  0.460894 -0.693495 -0.411975 -1.742215  0.167572   \n",
       "305204  0.701472  0.088044  0.365363 -0.797573 -0.335046 -1.749145  0.162281   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "487195 -0.308088 -0.902133 -0.788416 -0.455089 -0.965098 -0.731755  1.814973   \n",
       "487196 -0.303594 -0.903629 -0.788044 -0.471002 -0.972361 -0.740052  1.767146   \n",
       "487197 -0.297195 -0.909914 -0.786307 -0.484294 -0.982479 -0.748602  1.729157   \n",
       "487198 -0.290678 -0.922501 -0.782228 -0.496396 -0.994606 -0.753511  1.710593   \n",
       "487199 -0.286750 -0.940638 -0.773509 -0.511405 -1.008118 -0.752298  1.712542   \n",
       "\n",
       "        L3B_flex  L3C_flex  R1A_flex  ...  R1C_flex  R2A_flex  R2B_flex  \\\n",
       "305200  0.282390 -0.310062 -0.264968  ...  0.693084 -0.585737 -1.094131   \n",
       "305201  0.172505 -0.249874 -0.271291  ...  0.694916 -0.579001 -1.113678   \n",
       "305202  0.132534 -0.203036 -0.267053  ...  0.664057 -0.530863 -1.159401   \n",
       "305203  0.133926 -0.180630 -0.235028  ...  0.604496 -0.430864 -1.228570   \n",
       "305204  0.143477 -0.183611 -0.192129  ...  0.541270 -0.281702 -1.303932   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "487195 -0.622985 -0.564088  1.669204  ...  0.589357 -1.684049  0.313072   \n",
       "487196 -0.655100 -0.613778  1.632109  ...  0.533910 -1.358959  0.320817   \n",
       "487197 -0.692736 -0.674416  1.598554  ...  0.495282 -0.973195  0.329923   \n",
       "487198 -0.723309 -0.729794  1.569301  ...  0.466783 -0.970619  0.335979   \n",
       "487199 -0.740850 -0.771067  1.545161  ...  0.444080 -1.614606  0.335653   \n",
       "\n",
       "        R2C_flex  R3A_flex  R3B_flex  R3C_flex     x_vel     y_vel     z_vel  \n",
       "305200 -1.513160  1.320046 -0.004703  0.497005  1.096183  2.616862  5.257112  \n",
       "305201 -1.506804  0.212285 -0.272115  0.589078  1.186000  2.676443  5.157295  \n",
       "305202 -1.499572 -0.320635 -0.432969  0.692877  1.273659  2.689129  5.039348  \n",
       "305203 -1.493661 -0.468435 -0.493286  0.799473  1.358636  2.643880  4.900506  \n",
       "305204 -1.493818 -0.461853 -0.511329  0.897564  1.440223  2.531357  4.738617  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "487195  0.856602 -0.335242  2.837208  2.588538 -0.428126  0.975551  0.811247  \n",
       "487196  0.854324 -0.325824  2.827270  2.594308 -0.656853  1.225061  0.704523  \n",
       "487197  0.847678 -0.320627  2.818182  2.595073 -0.860701  1.480759  0.588388  \n",
       "487198  0.857931 -0.321187  2.814204  2.593385 -1.038202  1.732315  0.469502  \n",
       "487199  0.899181 -0.328250  2.816906  2.590686 -1.190549  1.972467  0.352829  \n",
       "\n",
       "[182000 rows x 21 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['x_vel','y_vel','z_vel']\n",
    "#df = create_lagged_features(df, features, 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['x_vel','y_vel','z_vel']].values\n",
    "y = df.drop(columns=['x_vel','y_vel','z_vel']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerations shape: (181497, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'velocities' is your (72800, 3) velocity array\n",
    "# Define time step\n",
    "dt = 1  # Replace with actual time step if known\n",
    "velocities = X  # Your velocity array\n",
    "chunk_size = 600\n",
    "num_chunks = velocities.shape[0] // chunk_size  # Ensure complete chunks\n",
    "accelerations = []\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Get the current chunk\n",
    "    chunk = velocities[i * chunk_size: (i + 1) * chunk_size]\n",
    "    \n",
    "    # Calculate acceleration for the chunk\n",
    "    accel_chunk = np.diff(chunk, axis=0) / dt  # Shape will be (599, 3)\n",
    "    accelerations.append(accel_chunk)\n",
    "\n",
    "# Combine accelerations into a single array\n",
    "accelerations = np.vstack(accelerations)\n",
    "\n",
    "# Output the final shape\n",
    "print(\"Accelerations shape:\", accelerations.shape)  # Should be (599 * num_chunks, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim Angle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerations shape: (181497, 3)\n",
      "Trimmed output shape: (181497, 18)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming `accelerations` has shape (77870, 3) and `output_data` has shape (72800, 3)\n",
    "# We need to trim the output_data such that the first value from each chunk of 600 is removed\n",
    "output_data=y\n",
    "# Split the output_data into chunks of 600\n",
    "chunk_size = 600\n",
    "num_chunks = output_data.shape[0] // chunk_size\n",
    "\n",
    "# Create a new list for the trimmed output\n",
    "trimmed_output = []\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Get the current chunk (600 values)\n",
    "    chunk = output_data[i * chunk_size: (i + 1) * chunk_size]\n",
    "    \n",
    "    # Remove the first value of the chunk\n",
    "    trimmed_chunk = chunk[1:]  # This removes the first value in the chunk\n",
    "    trimmed_output.append(trimmed_chunk)\n",
    "\n",
    "# Stack the chunks back into a single array\n",
    "trimmed_output = np.vstack(trimmed_output)\n",
    "\n",
    "# Check the shapes\n",
    "print(\"Accelerations shape:\", accelerations.shape)  # Should be (77870, 3)\n",
    "print(\"Trimmed output shape:\", trimmed_output.shape)  # Should be (77870, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows (no overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trials: 100%|██████████| 303/303 [00:00<00:00, 3462.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input windows shape (X): (120897, 200, 3)\n",
      "Targets shape (y): (120897, 18)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to split the dataset into trials\n",
    "def split_into_trials(data, trial_size):\n",
    "    \"\"\"\n",
    "    Split a long dataset into trials of specified size.\n",
    "    Args:\n",
    "        data (array): The dataset as a 2D array (frames x features).\n",
    "        trial_size (int): Number of frames per trial.\n",
    "    Returns:\n",
    "        list: A list of trials, each a 2D array (trial_size x features).\n",
    "    \"\"\"\n",
    "    return [data[i:i + trial_size] for i in range(0, len(data), trial_size)]\n",
    "\n",
    "# Function to create past windows for each trial\n",
    "def create_past_windows_per_trial(trials, targets, window_size, stride):\n",
    "    \"\"\"\n",
    "    Generate overlapping or non-overlapping windows for input features and targets.\n",
    "    Args:\n",
    "        trials (list of arrays): List of 2D feature arrays (frames x features).\n",
    "        targets (list of arrays): List of 2D target arrays (frames x target_dims).\n",
    "        window_size (int): Number of frames in each window.\n",
    "        stride (int): Step size for the sliding window.\n",
    "    Returns:\n",
    "        tuple: (X, y), where X contains input windows and y contains targets.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for trial, target in tqdm(zip(trials, targets), desc=\"Processing trials\", total=len(trials)):\n",
    "        for i in range(0, len(trial) - window_size, stride):\n",
    "            window = trial[i:i + window_size, :]  \n",
    "            target_window = target[i + window_size, :] \n",
    "            X.append(window)\n",
    "            y.append(target_window)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "trial_size = 599  \n",
    "window_size = 200  \n",
    "stride = 1  \n",
    "\n",
    " \n",
    "trials_features = split_into_trials(accelerations, trial_size)\n",
    "trials_targets = split_into_trials(trimmed_output, trial_size)\n",
    "\n",
    " \n",
    "X, y = create_past_windows_per_trial(trials_features, trials_targets, window_size, stride)\n",
    "\n",
    "   \n",
    "print(f\"Input windows shape (X): {X.shape}\")  \n",
    "print(f\"Targets shape (y): {y.shape}\")        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows (with overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181297/181297 [00:00<00:00, 1303454.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((181297, 200, 3), (181297, 18))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_past_windows(features, targets, window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in tqdm(range(len(features) - window_size)):\n",
    "        # Window of past data points\n",
    "        window = features[i:i + window_size]  # Past values leading up to the target\n",
    "        target_value = targets[i + window_size]  # Target is the current value after the window\n",
    "        X.append(window)\n",
    "        y.append(target_value)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define window size\n",
    "window_size = 200\n",
    "\n",
    "# Apply windowing to data_P9LT and xf_transformed\n",
    "X, y = create_past_windows(accelerations, trimmed_output, window_size)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    # First LSTM layer\n",
    "    LSTM(128, return_sequences=True, input_shape=(window_size, 3)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Second LSTM layer\n",
    "    LSTM(256, return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Third LSTM layer\n",
    "    LSTM(512, return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Fourth LSTM layer\n",
    "    LSTM(256, return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Dense layers\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(18, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, BatchNormalization\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    # First Bidirectional LSTM layer\n",
    "    Bidirectional(LSTM(128, return_sequences=True), input_shape=(window_size, 3)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Second Bidirectional LSTM layer\n",
    "    Bidirectional(LSTM(256, return_sequences=True)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Third Bidirectional LSTM layer\n",
    "    Bidirectional(LSTM(512, return_sequences=True)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Fourth Bidirectional LSTM layer\n",
    "    Bidirectional(LSTM(256, return_sequences=False)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Dense layers\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(18, activation='linear')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def detect_peaks(sequence, threshold=0.5, distance=5):\n",
    "    peaks, _ = find_peaks(sequence, height=threshold, distance=distance)\n",
    "    return peaks\n",
    "\n",
    "\n",
    "def peak_detection_accuracy(y_true, y_pred, threshold=0.5, distance=5):\n",
    "\n",
    "    y_true = tf.keras.backend.get_value(y_true)\n",
    "    y_pred = tf.keras.backend.get_value(y_pred)\n",
    "\n",
    "\n",
    "    true_peaks = [detect_peaks(y, threshold, distance) for y in y_true]\n",
    "    pred_peaks = [detect_peaks(y, threshold, distance) for y in y_pred]\n",
    "\n",
    "  \n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "\n",
    "    for tp, pp in zip(true_peaks, pred_peaks):\n",
    "        true_positive += len(np.intersect1d(tp, pp))  \n",
    "        false_positive += len(np.setdiff1d(pp, tp))   \n",
    "        false_negative += len(np.setdiff1d(tp, pp))  \n",
    "\n",
    "\n",
    "    precision = true_positive / (true_positive + false_positive + 1e-6)\n",
    "    recall = true_positive / (true_positive + false_negative + 1e-6)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "\n",
    "def peak_detection_metric(y_true, y_pred):\n",
    "    return tf.py_function(peak_detection_accuracy, [y_true, y_pred], tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Output Peak Minima "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "\n",
    "def find_peaks_per_output(true_values, prominence):\n",
    "    num_outputs = true_values.shape[1]\n",
    "    batch_peaks = []\n",
    "\n",
    "    for i in range(num_outputs):\n",
    "        true_values_i = true_values[:, i]\n",
    "        if isinstance(true_values_i, tf.Tensor):\n",
    "            # Ensure that the tensor is in eager execution mode to call numpy\n",
    "            true_values_np = true_values_i.numpy()  # Directly use .numpy() if in eager mode\n",
    "        else:\n",
    "            true_values_np = true_values_i  # Assume it's already a NumPy array\n",
    "\n",
    "        peaks, _ = find_peaks(true_values_np, prominence=prominence)\n",
    "        batch_peaks.append(peaks)\n",
    "\n",
    "    return batch_peaks\n",
    "\n",
    "\n",
    "def find_minima_by_derivative(true_values):\n",
    "    num_outputs = true_values.shape[1]\n",
    "    batch_minima = []\n",
    "\n",
    "    for i in range(num_outputs):\n",
    "        # Compute first derivative\n",
    "        diff = np.diff(true_values[:, i])\n",
    "        \n",
    "        # Find where the slope changes from negative to positive\n",
    "        minima_indices = np.where((diff[:-1] < 0) & (diff[1:] > 0))[0] + 1\n",
    "        \n",
    "        batch_minima.append(minima_indices)\n",
    "\n",
    "    return batch_minima\n",
    "\n",
    "\n",
    "def multi_output_peak_minima_loss_function(y_true, y_pred, prominence, peak_weight=0.3, minima_weight=0.3):\n",
    "    num_outputs = y_true.shape[1]  # Assuming y_true has shape (batch_size, num_outputs)\n",
    "    total_loss = 0.0\n",
    "    count_outputs = 0\n",
    "\n",
    "    for i in range(num_outputs):\n",
    "        # Find peaks in the true values\n",
    "        peak_indices, _ = find_peaks(y_true[:, i], prominence=prominence)  # Get indices of peaks\n",
    "        \n",
    "        # Find minima in the true values\n",
    "        minima_indices = find_minima_by_derivative(y_true)  # Custom minima finder based on derivative\n",
    "        \n",
    "        # Calculate MAE for all points\n",
    "        mae_loss_all = tf.reduce_mean(tf.abs(y_true[:, i] - y_pred[:, i]))\n",
    "\n",
    "        # Calculate MAE for peaks only\n",
    "        if len(peak_indices) > 0:\n",
    "            peak_mask = np.zeros(y_true[:, i].shape, dtype=bool)\n",
    "            peak_mask[peak_indices] = True\n",
    "            peak_true = tf.boolean_mask(y_true[:, i], peak_mask)\n",
    "            peak_pred = tf.boolean_mask(y_pred[:, i], peak_mask)\n",
    "            peak_mae_loss = tf.reduce_mean(tf.abs(peak_true - peak_pred))\n",
    "        else:\n",
    "            peak_mae_loss = 0.0  # No peaks found\n",
    "\n",
    "        # Calculate MAE for minima only\n",
    "        if len(minima_indices[i]) > 0:\n",
    "            minima_mask = np.zeros(y_true[:, i].shape, dtype=bool)\n",
    "            minima_mask[minima_indices[i]] = True\n",
    "            minima_true = tf.boolean_mask(y_true[:, i], minima_mask)\n",
    "            minima_pred = tf.boolean_mask(y_pred[:, i], minima_mask)\n",
    "            minima_mae_loss = tf.reduce_mean(tf.abs(minima_true - minima_pred))\n",
    "        else:\n",
    "            minima_mae_loss = 0.0  # No minima found\n",
    "\n",
    "        # Combine the losses with weight\n",
    "        total_loss += mae_loss_all + (peak_weight * peak_mae_loss) + (minima_weight * minima_mae_loss)\n",
    "        count_outputs += 1\n",
    "\n",
    "    # Average the total loss across all outputs\n",
    "    return total_loss / tf.maximum(tf.cast(count_outputs, tf.float32), 1.0)\n",
    "\n",
    "\n",
    "# Example usage in model compilation\n",
    "lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                   loss=lambda y_true, y_pred: multi_output_peak_minima_loss_function(y_true, y_pred, prominence=0.5), metrics=[peak_detection_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile(loss=tf.keras.losses.mae,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=[peak_detection_metric]\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n",
      "162/162 [==============================] - 82s 446ms/step - loss: 0.9831 - peak_detection_metric: 0.0824 - val_loss: 0.7394 - val_peak_detection_metric: 0.0012\n",
      "Epoch 2/4000\n",
      "162/162 [==============================] - 71s 438ms/step - loss: 0.8118 - peak_detection_metric: 0.0359 - val_loss: 0.7132 - val_peak_detection_metric: 0.0194\n",
      "Epoch 3/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.7767 - peak_detection_metric: 0.0660 - val_loss: 0.7587 - val_peak_detection_metric: 0.0338\n",
      "Epoch 4/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.7424 - peak_detection_metric: 0.1258 - val_loss: 0.7158 - val_peak_detection_metric: 0.0432\n",
      "Epoch 5/4000\n",
      "162/162 [==============================] - 69s 423ms/step - loss: 0.7105 - peak_detection_metric: 0.1893 - val_loss: 0.7260 - val_peak_detection_metric: 0.0775\n",
      "Epoch 6/4000\n",
      "162/162 [==============================] - 68s 423ms/step - loss: 0.6751 - peak_detection_metric: 0.2403 - val_loss: 0.7304 - val_peak_detection_metric: 0.0711\n",
      "Epoch 7/4000\n",
      "162/162 [==============================] - 68s 423ms/step - loss: 0.6445 - peak_detection_metric: 0.2711 - val_loss: 0.7357 - val_peak_detection_metric: 0.1172\n",
      "Epoch 8/4000\n",
      "162/162 [==============================] - 68s 423ms/step - loss: 0.6159 - peak_detection_metric: 0.3006 - val_loss: 0.7485 - val_peak_detection_metric: 0.1214\n",
      "Epoch 9/4000\n",
      "162/162 [==============================] - 69s 425ms/step - loss: 0.5936 - peak_detection_metric: 0.3241 - val_loss: 0.7612 - val_peak_detection_metric: 0.1430\n",
      "Epoch 10/4000\n",
      "162/162 [==============================] - 68s 423ms/step - loss: 0.5760 - peak_detection_metric: 0.3389 - val_loss: 0.7689 - val_peak_detection_metric: 0.1095\n",
      "Epoch 11/4000\n",
      "162/162 [==============================] - 69s 423ms/step - loss: 0.5660 - peak_detection_metric: 0.3510 - val_loss: 0.7642 - val_peak_detection_metric: 0.1451\n",
      "Epoch 12/4000\n",
      "162/162 [==============================] - 69s 423ms/step - loss: 0.5464 - peak_detection_metric: 0.3697 - val_loss: 0.7569 - val_peak_detection_metric: 0.1399\n",
      "Epoch 13/4000\n",
      "162/162 [==============================] - 69s 423ms/step - loss: 0.5349 - peak_detection_metric: 0.3795 - val_loss: 0.7797 - val_peak_detection_metric: 0.1373\n",
      "Epoch 14/4000\n",
      "162/162 [==============================] - 69s 424ms/step - loss: 0.5233 - peak_detection_metric: 0.3906 - val_loss: 0.7660 - val_peak_detection_metric: 0.1422\n",
      "Epoch 15/4000\n",
      "162/162 [==============================] - 69s 423ms/step - loss: 0.5122 - peak_detection_metric: 0.3998 - val_loss: 0.7906 - val_peak_detection_metric: 0.1468\n",
      "Epoch 16/4000\n",
      "162/162 [==============================] - 69s 424ms/step - loss: 0.5026 - peak_detection_metric: 0.4065 - val_loss: 0.7858 - val_peak_detection_metric: 0.1558\n",
      "Epoch 17/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4937 - peak_detection_metric: 0.4131 - val_loss: 0.7820 - val_peak_detection_metric: 0.1245\n",
      "Epoch 18/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4916 - peak_detection_metric: 0.4142 - val_loss: 0.7973 - val_peak_detection_metric: 0.1341\n",
      "Epoch 19/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4960 - peak_detection_metric: 0.4116 - val_loss: 0.7819 - val_peak_detection_metric: 0.1595\n",
      "Epoch 20/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4772 - peak_detection_metric: 0.4270 - val_loss: 0.7572 - val_peak_detection_metric: 0.1618\n",
      "Epoch 21/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4726 - peak_detection_metric: 0.4298 - val_loss: 0.7513 - val_peak_detection_metric: 0.1728\n",
      "Epoch 22/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4679 - peak_detection_metric: 0.4358 - val_loss: 0.7657 - val_peak_detection_metric: 0.1651\n",
      "Epoch 23/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4618 - peak_detection_metric: 0.4384 - val_loss: 0.7462 - val_peak_detection_metric: 0.1672\n",
      "Epoch 24/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4661 - peak_detection_metric: 0.4388 - val_loss: 0.7697 - val_peak_detection_metric: 0.1486\n",
      "Epoch 25/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.4563 - peak_detection_metric: 0.4470 - val_loss: 0.7655 - val_peak_detection_metric: 0.1513\n",
      "Epoch 26/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4515 - peak_detection_metric: 0.4526 - val_loss: 0.7784 - val_peak_detection_metric: 0.1511\n",
      "Epoch 27/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4507 - peak_detection_metric: 0.4544 - val_loss: 0.7575 - val_peak_detection_metric: 0.1800\n",
      "Epoch 28/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4483 - peak_detection_metric: 0.4558 - val_loss: 0.7555 - val_peak_detection_metric: 0.1787\n",
      "Epoch 29/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4422 - peak_detection_metric: 0.4621 - val_loss: 0.7839 - val_peak_detection_metric: 0.1610\n",
      "Epoch 30/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4388 - peak_detection_metric: 0.4653 - val_loss: 0.7619 - val_peak_detection_metric: 0.1767\n",
      "Epoch 31/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4359 - peak_detection_metric: 0.4671 - val_loss: 0.7578 - val_peak_detection_metric: 0.1712\n",
      "Epoch 32/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4355 - peak_detection_metric: 0.4687 - val_loss: 0.7997 - val_peak_detection_metric: 0.1640\n",
      "Epoch 33/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4328 - peak_detection_metric: 0.4697 - val_loss: 0.7682 - val_peak_detection_metric: 0.1643\n",
      "Epoch 34/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4387 - peak_detection_metric: 0.4663 - val_loss: 0.8041 - val_peak_detection_metric: 0.1442\n",
      "Epoch 35/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4422 - peak_detection_metric: 0.4624 - val_loss: 0.8081 - val_peak_detection_metric: 0.1490\n",
      "Epoch 36/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4323 - peak_detection_metric: 0.4748 - val_loss: 0.7678 - val_peak_detection_metric: 0.1709\n",
      "Epoch 37/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4269 - peak_detection_metric: 0.4765 - val_loss: 0.7731 - val_peak_detection_metric: 0.1660\n",
      "Epoch 38/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4234 - peak_detection_metric: 0.4797 - val_loss: 0.7846 - val_peak_detection_metric: 0.1587\n",
      "Epoch 39/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4221 - peak_detection_metric: 0.4840 - val_loss: 0.7661 - val_peak_detection_metric: 0.1786\n",
      "Epoch 40/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.4209 - peak_detection_metric: 0.4833 - val_loss: 0.8363 - val_peak_detection_metric: 0.1575\n",
      "Epoch 41/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4193 - peak_detection_metric: 0.4857 - val_loss: 0.7702 - val_peak_detection_metric: 0.1818\n",
      "Epoch 42/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4179 - peak_detection_metric: 0.4861 - val_loss: 0.7984 - val_peak_detection_metric: 0.1554\n",
      "Epoch 43/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4484 - peak_detection_metric: 0.4609 - val_loss: 0.8040 - val_peak_detection_metric: 0.1654\n",
      "Epoch 44/4000\n",
      "162/162 [==============================] - 70s 430ms/step - loss: 0.4241 - peak_detection_metric: 0.4820 - val_loss: 0.7888 - val_peak_detection_metric: 0.1527\n",
      "Epoch 45/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4160 - peak_detection_metric: 0.4887 - val_loss: 0.7672 - val_peak_detection_metric: 0.1737\n",
      "Epoch 46/4000\n",
      "162/162 [==============================] - 68s 423ms/step - loss: 0.4139 - peak_detection_metric: 0.4914 - val_loss: 0.7826 - val_peak_detection_metric: 0.1609\n",
      "Epoch 47/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4141 - peak_detection_metric: 0.4914 - val_loss: 0.7805 - val_peak_detection_metric: 0.1595\n",
      "Epoch 48/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.4107 - peak_detection_metric: 0.4947 - val_loss: 0.7771 - val_peak_detection_metric: 0.1661\n",
      "Epoch 49/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4097 - peak_detection_metric: 0.4958 - val_loss: 0.8081 - val_peak_detection_metric: 0.1354\n",
      "Epoch 50/4000\n",
      "162/162 [==============================] - 69s 425ms/step - loss: 0.4087 - peak_detection_metric: 0.4965 - val_loss: 0.7756 - val_peak_detection_metric: 0.1706\n",
      "Epoch 51/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4076 - peak_detection_metric: 0.4967 - val_loss: 0.7779 - val_peak_detection_metric: 0.1878\n",
      "Epoch 52/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4065 - peak_detection_metric: 0.4961 - val_loss: 0.7783 - val_peak_detection_metric: 0.1899\n",
      "Epoch 53/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4134 - peak_detection_metric: 0.4921 - val_loss: 0.8104 - val_peak_detection_metric: 0.1708\n",
      "Epoch 54/4000\n",
      "162/162 [==============================] - 68s 423ms/step - loss: 0.4122 - peak_detection_metric: 0.4936 - val_loss: 0.7969 - val_peak_detection_metric: 0.1689\n",
      "Epoch 55/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4072 - peak_detection_metric: 0.4984 - val_loss: 0.7742 - val_peak_detection_metric: 0.1598\n",
      "Epoch 56/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4050 - peak_detection_metric: 0.4993 - val_loss: 0.7565 - val_peak_detection_metric: 0.1915\n",
      "Epoch 57/4000\n",
      "162/162 [==============================] - 69s 423ms/step - loss: 0.4049 - peak_detection_metric: 0.4991 - val_loss: 0.7626 - val_peak_detection_metric: 0.1738\n",
      "Epoch 58/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4016 - peak_detection_metric: 0.5037 - val_loss: 0.7727 - val_peak_detection_metric: 0.1722\n",
      "Epoch 59/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4013 - peak_detection_metric: 0.5026 - val_loss: 0.7961 - val_peak_detection_metric: 0.1755\n",
      "Epoch 60/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.4075 - peak_detection_metric: 0.5007 - val_loss: 0.7659 - val_peak_detection_metric: 0.1696\n",
      "Epoch 61/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4022 - peak_detection_metric: 0.5038 - val_loss: 0.7849 - val_peak_detection_metric: 0.1873\n",
      "Epoch 62/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.4000 - peak_detection_metric: 0.5051 - val_loss: 0.8088 - val_peak_detection_metric: 0.1632\n",
      "Epoch 63/4000\n",
      "162/162 [==============================] - 69s 424ms/step - loss: 0.3986 - peak_detection_metric: 0.5054 - val_loss: 0.8443 - val_peak_detection_metric: 0.1554\n",
      "Epoch 64/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3972 - peak_detection_metric: 0.5072 - val_loss: 0.7816 - val_peak_detection_metric: 0.1431\n",
      "Epoch 65/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3960 - peak_detection_metric: 0.5098 - val_loss: 0.7729 - val_peak_detection_metric: 0.1726\n",
      "Epoch 66/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3966 - peak_detection_metric: 0.5099 - val_loss: 0.7963 - val_peak_detection_metric: 0.1738\n",
      "Epoch 67/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3961 - peak_detection_metric: 0.5084 - val_loss: 0.7739 - val_peak_detection_metric: 0.1685\n",
      "Epoch 68/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3969 - peak_detection_metric: 0.5076 - val_loss: 0.7889 - val_peak_detection_metric: 0.1446\n",
      "Epoch 69/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3946 - peak_detection_metric: 0.5098 - val_loss: 0.7863 - val_peak_detection_metric: 0.1750\n",
      "Epoch 70/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3987 - peak_detection_metric: 0.5050 - val_loss: 0.7684 - val_peak_detection_metric: 0.1905\n",
      "Epoch 71/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3927 - peak_detection_metric: 0.5083 - val_loss: 0.7707 - val_peak_detection_metric: 0.1740\n",
      "Epoch 72/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3922 - peak_detection_metric: 0.5121 - val_loss: 0.7666 - val_peak_detection_metric: 0.1852\n",
      "Epoch 73/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3980 - peak_detection_metric: 0.5101 - val_loss: 0.8407 - val_peak_detection_metric: 0.1774\n",
      "Epoch 74/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3942 - peak_detection_metric: 0.5084 - val_loss: 0.7761 - val_peak_detection_metric: 0.1765\n",
      "Epoch 75/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3899 - peak_detection_metric: 0.5142 - val_loss: 0.7913 - val_peak_detection_metric: 0.1746\n",
      "Epoch 76/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3890 - peak_detection_metric: 0.5141 - val_loss: 0.7675 - val_peak_detection_metric: 0.1767\n",
      "Epoch 77/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3878 - peak_detection_metric: 0.5163 - val_loss: 0.7704 - val_peak_detection_metric: 0.1810\n",
      "Epoch 78/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3873 - peak_detection_metric: 0.5163 - val_loss: 0.7761 - val_peak_detection_metric: 0.1747\n",
      "Epoch 79/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3859 - peak_detection_metric: 0.5164 - val_loss: 0.7694 - val_peak_detection_metric: 0.1599\n",
      "Epoch 80/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3856 - peak_detection_metric: 0.5153 - val_loss: 0.7812 - val_peak_detection_metric: 0.1754\n",
      "Epoch 81/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3856 - peak_detection_metric: 0.5156 - val_loss: 0.7908 - val_peak_detection_metric: 0.1743\n",
      "Epoch 82/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3854 - peak_detection_metric: 0.5178 - val_loss: 0.7771 - val_peak_detection_metric: 0.1760\n",
      "Epoch 83/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3847 - peak_detection_metric: 0.5167 - val_loss: 0.7750 - val_peak_detection_metric: 0.1694\n",
      "Epoch 84/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3863 - peak_detection_metric: 0.5169 - val_loss: 0.7477 - val_peak_detection_metric: 0.1460\n",
      "Epoch 85/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.4018 - peak_detection_metric: 0.5035 - val_loss: 0.7857 - val_peak_detection_metric: 0.1778\n",
      "Epoch 86/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3861 - peak_detection_metric: 0.5169 - val_loss: 0.7659 - val_peak_detection_metric: 0.1796\n",
      "Epoch 87/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3837 - peak_detection_metric: 0.5176 - val_loss: 0.7942 - val_peak_detection_metric: 0.1780\n",
      "Epoch 88/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3823 - peak_detection_metric: 0.5221 - val_loss: 0.7969 - val_peak_detection_metric: 0.1570\n",
      "Epoch 89/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3813 - peak_detection_metric: 0.5214 - val_loss: 0.7824 - val_peak_detection_metric: 0.1820\n",
      "Epoch 90/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3819 - peak_detection_metric: 0.5194 - val_loss: 0.7970 - val_peak_detection_metric: 0.1660\n",
      "Epoch 91/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3848 - peak_detection_metric: 0.5188 - val_loss: 0.7959 - val_peak_detection_metric: 0.1682\n",
      "Epoch 92/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3813 - peak_detection_metric: 0.5208 - val_loss: 0.7880 - val_peak_detection_metric: 0.1813\n",
      "Epoch 93/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3921 - peak_detection_metric: 0.5115 - val_loss: 0.7896 - val_peak_detection_metric: 0.1802\n",
      "Epoch 94/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3810 - peak_detection_metric: 0.5198 - val_loss: 0.7572 - val_peak_detection_metric: 0.1952\n",
      "Epoch 95/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3792 - peak_detection_metric: 0.5221 - val_loss: 0.8129 - val_peak_detection_metric: 0.1708\n",
      "Epoch 96/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3783 - peak_detection_metric: 0.5232 - val_loss: 0.8174 - val_peak_detection_metric: 0.1741\n",
      "Epoch 97/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3771 - peak_detection_metric: 0.5238 - val_loss: 0.7803 - val_peak_detection_metric: 0.1761\n",
      "Epoch 98/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3770 - peak_detection_metric: 0.5236 - val_loss: 0.7723 - val_peak_detection_metric: 0.1778\n",
      "Epoch 99/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3786 - peak_detection_metric: 0.5248 - val_loss: 0.7744 - val_peak_detection_metric: 0.1757\n",
      "Epoch 100/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3760 - peak_detection_metric: 0.5244 - val_loss: 0.7821 - val_peak_detection_metric: 0.1703\n",
      "Epoch 101/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3755 - peak_detection_metric: 0.5265 - val_loss: 0.7749 - val_peak_detection_metric: 0.1724\n",
      "Epoch 102/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3747 - peak_detection_metric: 0.5274 - val_loss: 0.7690 - val_peak_detection_metric: 0.1691\n",
      "Epoch 103/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3752 - peak_detection_metric: 0.5272 - val_loss: 0.7897 - val_peak_detection_metric: 0.1897\n",
      "Epoch 104/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3917 - peak_detection_metric: 0.5121 - val_loss: 0.8038 - val_peak_detection_metric: 0.1516\n",
      "Epoch 105/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.4099 - peak_detection_metric: 0.5028 - val_loss: 0.7772 - val_peak_detection_metric: 0.1588\n",
      "Epoch 106/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3865 - peak_detection_metric: 0.5149 - val_loss: 0.7883 - val_peak_detection_metric: 0.1626\n",
      "Epoch 107/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3808 - peak_detection_metric: 0.5233 - val_loss: 0.7952 - val_peak_detection_metric: 0.1628\n",
      "Epoch 108/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3774 - peak_detection_metric: 0.5262 - val_loss: 0.7692 - val_peak_detection_metric: 0.1791\n",
      "Epoch 109/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3740 - peak_detection_metric: 0.5271 - val_loss: 0.7757 - val_peak_detection_metric: 0.1768\n",
      "Epoch 110/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3730 - peak_detection_metric: 0.5290 - val_loss: 0.7790 - val_peak_detection_metric: 0.1763\n",
      "Epoch 111/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3722 - peak_detection_metric: 0.5284 - val_loss: 0.7869 - val_peak_detection_metric: 0.1732\n",
      "Epoch 112/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3717 - peak_detection_metric: 0.5309 - val_loss: 0.7642 - val_peak_detection_metric: 0.1756\n",
      "Epoch 113/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3715 - peak_detection_metric: 0.5290 - val_loss: 0.7661 - val_peak_detection_metric: 0.1788\n",
      "Epoch 114/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3710 - peak_detection_metric: 0.5295 - val_loss: 0.7782 - val_peak_detection_metric: 0.1717\n",
      "Epoch 115/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3705 - peak_detection_metric: 0.5292 - val_loss: 0.7842 - val_peak_detection_metric: 0.1834\n",
      "Epoch 116/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3705 - peak_detection_metric: 0.5300 - val_loss: 0.7727 - val_peak_detection_metric: 0.1832\n",
      "Epoch 117/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3702 - peak_detection_metric: 0.5314 - val_loss: 0.7729 - val_peak_detection_metric: 0.1837\n",
      "Epoch 118/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3708 - peak_detection_metric: 0.5298 - val_loss: 0.7677 - val_peak_detection_metric: 0.1696\n",
      "Epoch 119/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3693 - peak_detection_metric: 0.5324 - val_loss: 0.7645 - val_peak_detection_metric: 0.1712\n",
      "Epoch 120/4000\n",
      "162/162 [==============================] - 68s 420ms/step - loss: 0.3708 - peak_detection_metric: 0.5306 - val_loss: 0.7657 - val_peak_detection_metric: 0.1696\n",
      "Epoch 121/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3695 - peak_detection_metric: 0.5313 - val_loss: 0.7867 - val_peak_detection_metric: 0.1697\n",
      "Epoch 122/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3692 - peak_detection_metric: 0.5308 - val_loss: 0.7747 - val_peak_detection_metric: 0.1749\n",
      "Epoch 123/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3681 - peak_detection_metric: 0.5326 - val_loss: 0.7768 - val_peak_detection_metric: 0.1663\n",
      "Epoch 124/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3681 - peak_detection_metric: 0.5325 - val_loss: 0.7850 - val_peak_detection_metric: 0.1663\n",
      "Epoch 125/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3683 - peak_detection_metric: 0.5332 - val_loss: 0.7659 - val_peak_detection_metric: 0.1819\n",
      "Epoch 126/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3671 - peak_detection_metric: 0.5327 - val_loss: 0.7820 - val_peak_detection_metric: 0.1768\n",
      "Epoch 127/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3675 - peak_detection_metric: 0.5329 - val_loss: 0.7874 - val_peak_detection_metric: 0.1708\n",
      "Epoch 128/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3664 - peak_detection_metric: 0.5332 - val_loss: 0.7939 - val_peak_detection_metric: 0.1761\n",
      "Epoch 129/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3665 - peak_detection_metric: 0.5361 - val_loss: 0.7812 - val_peak_detection_metric: 0.1778\n",
      "Epoch 130/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.4075 - peak_detection_metric: 0.5012 - val_loss: 0.7987 - val_peak_detection_metric: 0.1711\n",
      "Epoch 131/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3981 - peak_detection_metric: 0.5083 - val_loss: 0.7812 - val_peak_detection_metric: 0.1807\n",
      "Epoch 132/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3714 - peak_detection_metric: 0.5288 - val_loss: 0.7790 - val_peak_detection_metric: 0.1654\n",
      "Epoch 133/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3693 - peak_detection_metric: 0.5300 - val_loss: 0.7806 - val_peak_detection_metric: 0.1733\n",
      "Epoch 134/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3672 - peak_detection_metric: 0.5329 - val_loss: 0.7649 - val_peak_detection_metric: 0.1769\n",
      "Epoch 135/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3654 - peak_detection_metric: 0.5328 - val_loss: 0.7681 - val_peak_detection_metric: 0.1807\n",
      "Epoch 136/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3648 - peak_detection_metric: 0.5348 - val_loss: 0.7749 - val_peak_detection_metric: 0.1681\n",
      "Epoch 137/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3676 - peak_detection_metric: 0.5330 - val_loss: 0.7599 - val_peak_detection_metric: 0.1780\n",
      "Epoch 138/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3651 - peak_detection_metric: 0.5324 - val_loss: 0.7793 - val_peak_detection_metric: 0.1832\n",
      "Epoch 139/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3645 - peak_detection_metric: 0.5314 - val_loss: 0.7609 - val_peak_detection_metric: 0.1940\n",
      "Epoch 140/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3639 - peak_detection_metric: 0.5353 - val_loss: 0.7812 - val_peak_detection_metric: 0.1581\n",
      "Epoch 141/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3634 - peak_detection_metric: 0.5372 - val_loss: 0.7743 - val_peak_detection_metric: 0.1870\n",
      "Epoch 142/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3647 - peak_detection_metric: 0.5350 - val_loss: 0.7679 - val_peak_detection_metric: 0.1803\n",
      "Epoch 143/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3639 - peak_detection_metric: 0.5367 - val_loss: 0.7808 - val_peak_detection_metric: 0.1755\n",
      "Epoch 144/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3698 - peak_detection_metric: 0.5320 - val_loss: 0.7693 - val_peak_detection_metric: 0.1818\n",
      "Epoch 145/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3645 - peak_detection_metric: 0.5344 - val_loss: 0.7807 - val_peak_detection_metric: 0.1811\n",
      "Epoch 146/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3695 - peak_detection_metric: 0.5299 - val_loss: 0.7696 - val_peak_detection_metric: 0.1768\n",
      "Epoch 147/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3649 - peak_detection_metric: 0.5347 - val_loss: 0.7652 - val_peak_detection_metric: 0.1826\n",
      "Epoch 148/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3636 - peak_detection_metric: 0.5359 - val_loss: 0.7745 - val_peak_detection_metric: 0.1858\n",
      "Epoch 149/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3630 - peak_detection_metric: 0.5348 - val_loss: 0.7629 - val_peak_detection_metric: 0.1986\n",
      "Epoch 150/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3618 - peak_detection_metric: 0.5373 - val_loss: 0.7768 - val_peak_detection_metric: 0.1825\n",
      "Epoch 151/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3621 - peak_detection_metric: 0.5383 - val_loss: 0.7479 - val_peak_detection_metric: 0.1757\n",
      "Epoch 152/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3741 - peak_detection_metric: 0.5267 - val_loss: 0.7823 - val_peak_detection_metric: 0.1703\n",
      "Epoch 153/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3812 - peak_detection_metric: 0.5233 - val_loss: 0.8190 - val_peak_detection_metric: 0.1347\n",
      "Epoch 154/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3674 - peak_detection_metric: 0.5339 - val_loss: 0.7738 - val_peak_detection_metric: 0.1769\n",
      "Epoch 155/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3634 - peak_detection_metric: 0.5353 - val_loss: 0.7725 - val_peak_detection_metric: 0.1853\n",
      "Epoch 156/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3625 - peak_detection_metric: 0.5378 - val_loss: 0.7767 - val_peak_detection_metric: 0.1826\n",
      "Epoch 157/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3618 - peak_detection_metric: 0.5383 - val_loss: 0.7812 - val_peak_detection_metric: 0.1810\n",
      "Epoch 158/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3637 - peak_detection_metric: 0.5357 - val_loss: 0.7761 - val_peak_detection_metric: 0.1949\n",
      "Epoch 159/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3613 - peak_detection_metric: 0.5365 - val_loss: 0.7741 - val_peak_detection_metric: 0.1827\n",
      "Epoch 160/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3599 - peak_detection_metric: 0.5387 - val_loss: 0.7732 - val_peak_detection_metric: 0.1867\n",
      "Epoch 161/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3603 - peak_detection_metric: 0.5380 - val_loss: 0.7661 - val_peak_detection_metric: 0.1878\n",
      "Epoch 162/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3598 - peak_detection_metric: 0.5400 - val_loss: 0.7753 - val_peak_detection_metric: 0.1898\n",
      "Epoch 163/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3601 - peak_detection_metric: 0.5409 - val_loss: 0.7728 - val_peak_detection_metric: 0.1726\n",
      "Epoch 164/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3588 - peak_detection_metric: 0.5416 - val_loss: 0.7788 - val_peak_detection_metric: 0.1791\n",
      "Epoch 165/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3588 - peak_detection_metric: 0.5430 - val_loss: 0.7681 - val_peak_detection_metric: 0.1926\n",
      "Epoch 166/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3591 - peak_detection_metric: 0.5394 - val_loss: 0.7781 - val_peak_detection_metric: 0.1848\n",
      "Epoch 167/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3591 - peak_detection_metric: 0.5388 - val_loss: 0.7737 - val_peak_detection_metric: 0.1795\n",
      "Epoch 168/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3596 - peak_detection_metric: 0.5406 - val_loss: 0.7733 - val_peak_detection_metric: 0.1875\n",
      "Epoch 169/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3616 - peak_detection_metric: 0.5377 - val_loss: 0.7685 - val_peak_detection_metric: 0.1923\n",
      "Epoch 170/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3798 - peak_detection_metric: 0.5237 - val_loss: 0.8095 - val_peak_detection_metric: 0.1932\n",
      "Epoch 171/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3717 - peak_detection_metric: 0.5278 - val_loss: 0.7835 - val_peak_detection_metric: 0.1855\n",
      "Epoch 172/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3622 - peak_detection_metric: 0.5381 - val_loss: 0.7650 - val_peak_detection_metric: 0.1847\n",
      "Epoch 173/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3598 - peak_detection_metric: 0.5377 - val_loss: 0.7787 - val_peak_detection_metric: 0.1768\n",
      "Epoch 174/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3596 - peak_detection_metric: 0.5401 - val_loss: 0.7778 - val_peak_detection_metric: 0.1840\n",
      "Epoch 175/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3587 - peak_detection_metric: 0.5407 - val_loss: 0.7673 - val_peak_detection_metric: 0.1808\n",
      "Epoch 176/4000\n",
      "162/162 [==============================] - 68s 422ms/step - loss: 0.3580 - peak_detection_metric: 0.5387 - val_loss: 0.7669 - val_peak_detection_metric: 0.1776\n",
      "Epoch 177/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3579 - peak_detection_metric: 0.5404 - val_loss: 0.7666 - val_peak_detection_metric: 0.1734\n",
      "Epoch 178/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3571 - peak_detection_metric: 0.5427 - val_loss: 0.7753 - val_peak_detection_metric: 0.1815\n",
      "Epoch 179/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3583 - peak_detection_metric: 0.5433 - val_loss: 0.7808 - val_peak_detection_metric: 0.1765\n",
      "Epoch 180/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3581 - peak_detection_metric: 0.5445 - val_loss: 0.7642 - val_peak_detection_metric: 0.1942\n",
      "Epoch 181/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3565 - peak_detection_metric: 0.5407 - val_loss: 0.7626 - val_peak_detection_metric: 0.1815\n",
      "Epoch 182/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3567 - peak_detection_metric: 0.5427 - val_loss: 0.7635 - val_peak_detection_metric: 0.1808\n",
      "Epoch 183/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3681 - peak_detection_metric: 0.5330 - val_loss: 0.7817 - val_peak_detection_metric: 0.1778\n",
      "Epoch 184/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3587 - peak_detection_metric: 0.5412 - val_loss: 0.7751 - val_peak_detection_metric: 0.1698\n",
      "Epoch 185/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3584 - peak_detection_metric: 0.5426 - val_loss: 0.7783 - val_peak_detection_metric: 0.1801\n",
      "Epoch 186/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3572 - peak_detection_metric: 0.5425 - val_loss: 0.7697 - val_peak_detection_metric: 0.1856\n",
      "Epoch 187/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3566 - peak_detection_metric: 0.5434 - val_loss: 0.7721 - val_peak_detection_metric: 0.1836\n",
      "Epoch 188/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3559 - peak_detection_metric: 0.5431 - val_loss: 0.7781 - val_peak_detection_metric: 0.1758\n",
      "Epoch 189/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3558 - peak_detection_metric: 0.5449 - val_loss: 0.7664 - val_peak_detection_metric: 0.1834\n",
      "Epoch 190/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3549 - peak_detection_metric: 0.5435 - val_loss: 0.7766 - val_peak_detection_metric: 0.1738\n",
      "Epoch 191/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3558 - peak_detection_metric: 0.5450 - val_loss: 0.7714 - val_peak_detection_metric: 0.1879\n",
      "Epoch 192/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3558 - peak_detection_metric: 0.5443 - val_loss: 0.7705 - val_peak_detection_metric: 0.1786\n",
      "Epoch 193/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3558 - peak_detection_metric: 0.5451 - val_loss: 0.7666 - val_peak_detection_metric: 0.1995\n",
      "Epoch 194/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3556 - peak_detection_metric: 0.5439 - val_loss: 0.7627 - val_peak_detection_metric: 0.1875\n",
      "Epoch 195/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3554 - peak_detection_metric: 0.5465 - val_loss: 0.7885 - val_peak_detection_metric: 0.1809\n",
      "Epoch 196/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3556 - peak_detection_metric: 0.5453 - val_loss: 0.7743 - val_peak_detection_metric: 0.1769\n",
      "Epoch 197/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3596 - peak_detection_metric: 0.5414 - val_loss: 0.7701 - val_peak_detection_metric: 0.1374\n",
      "Epoch 198/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3640 - peak_detection_metric: 0.5375 - val_loss: 0.7728 - val_peak_detection_metric: 0.1757\n",
      "Epoch 199/4000\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3772 - peak_detection_metric: 0.5288 - val_loss: 0.8164 - val_peak_detection_metric: 0.1690\n",
      "Epoch 200/4000\n",
      "162/162 [==============================] - ETA: 0s - loss: 0.3581 - peak_detection_metric: 0.5411Restoring model weights from the end of the best epoch: 190.\n",
      "162/162 [==============================] - 68s 421ms/step - loss: 0.3581 - peak_detection_metric: 0.5411 - val_loss: 0.7792 - val_peak_detection_metric: 0.1755\n",
      "Epoch 200: early stopping\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "# for ffn model\n",
    "#X = accelerations\n",
    "#y = trimmed_output\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_val = X[:split_index], X[split_index:-7200]\n",
    "y_train, y_val = y[:split_index], y[split_index:-7200]\n",
    "X_test = X[-7200:]\n",
    "y_test = y[-7200:]\n",
    "\n",
    "initial_epochs = 120  # Fixed number of epochs to train initially\n",
    "patience = 10        # Patience for early stopping after the initial epochs\n",
    "\n",
    "# Define custom callback for switching to early stopping after initial epochs\n",
    "class CustomEpochsAndPatience(Callback):\n",
    "    def __init__(self, initial_epochs):\n",
    "        super().__init__()\n",
    "        self.initial_epochs = initial_epochs\n",
    "        self.epochs_trained = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Track the number of epochs trained\n",
    "        self.epochs_trained += 1\n",
    "        # Stop training after the initial epochs if needed\n",
    "        if self.epochs_trained >= self.initial_epochs:\n",
    "            # If the number of epochs is met, enable early stopping callback\n",
    "            self.model.stop_training = False\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=patience, verbose=1, restore_best_weights=True)\n",
    "\n",
    "custom_callback = CustomEpochsAndPatience(initial_epochs=initial_epochs)\n",
    "\n",
    "history = lstm_model.fit(X_train, y_train, epochs=4000, callbacks=[custom_callback, early_stopping],batch_size=600, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 16s 61ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = (lstm_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 15s 65ms/step - loss: 0.8406 - peak_detection_metric: 0.1692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8406426310539246, 0.16920198500156403]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Directory to save plots\n",
    "dir_start = r\"C:\\\\Users\\\\bidayelab\\\\vel_to_angle_project\\\\acceleration\\\\bi_lstm\\\\\"\n",
    "dir_end = r\"\\(ReLU)activation_(fulltrial200past)window_size_(mae)loss_(peak_detection_metric)metric_(600)batch_size_(Adam0.001)optimizer\"\n",
    "save_dir = f\"{dir_start}preds{dir_end}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "chunk_size = 600\n",
    "num_chunks = len(y_test) // chunk_size\n",
    "num_features = y_test.shape[1]\n",
    "\n",
    "# Plot each feature in separate files with subplots for each chunk\n",
    "for feature_idx in range(num_features):\n",
    "    plt.figure(figsize=(15, 3 * num_chunks))\n",
    "    angle = df.columns.to_list()\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = start_idx + chunk_size\n",
    "        \n",
    "        plt.subplot(num_chunks, 1, i + 1)\n",
    "        plt.plot(y_test[start_idx:end_idx, feature_idx], label='True Values')\n",
    "        plt.plot(preds[start_idx:end_idx, feature_idx], label='Predictions')\n",
    "        plt.title(f'Trial {i + 1}')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Flex')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    \n",
    "    plt.suptitle(f'{angle[feature_idx]} - True Values vs Predictions', y=1.05)\n",
    "    file_path = os.path.join(save_dir, f'{angle[feature_idx]}.png')\n",
    "    plt.savefig(file_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume 'history' is the object returned by model.fit()\n",
    "# Replace 'f1' with the actual key you used in your model's metrics\n",
    "f1_scores = history.history.get('peak_detection_metric', [])\n",
    "val_f1_scores = history.history.get('val_peak_detection_metric', [])\n",
    "\n",
    "\n",
    "f1_save_dir = f\"{dir_start}F1_plots{dir_end}.png\"\n",
    "\n",
    "# Plot the training F1 score\n",
    "plt.plot(f1_scores, label='Training F1')\n",
    "\n",
    "plt.plot(val_f1_scores, label='Validation F1')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.title('F1 Score Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f1_save_dir, dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.save(f\"{dir_start}models{dir_end}.h5\")\n",
    "\n",
    "#loaded_model = tf.keras.models.load_model(f\"{dir_start}models{dir_end}.h5\",custom_objects={'peak_detection_metric': peak_detection_metric})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
